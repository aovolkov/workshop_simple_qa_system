{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"notebookId":"7807d185-4b39-47e9-b929-27775a3a58c1","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"ydsNotebookPath":"notebooks/1_prepare_data.ipynb"},"cells":[{"cell_type":"markdown","source":"## Импорт библиотек","metadata":{"cellId":"ff85c56o9esh4kule0xrkb"}},{"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nimport sys\nfrom glob import glob\nfrom os.path import join\nimport os\nimport re\nimport multiprocessing as mp\nfrom joblib import Parallel, delayed\nimport numpy as np\n%pip install lxml\n\nnp.random.seed(42)","metadata":{"cellId":"pm1vpb7yr8qs6mrooap09"},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: lxml in /home/jupyter/.local/lib/python3.7/site-packages (4.6.3)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":2},{"cell_type":"markdown","source":"## Парсинг телеграм чата","metadata":{"cellId":"6cz0ckpeinbp4stexi0or"}},{"cell_type":"code","source":"def get_str_from_file(path_to_file: str) -> str:\n    '''Функция для полуения строки из текстового файла'''\n    with open(path_to_file) as f:\n        contents = f.readlines()\n    text = ' '.join(content.strip() for content in contents).strip()\n    # удаляем ссылки из сообщений\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    return text\n\ndef read_html(path_to_html: str):\n    '''Функция для чтения html'''\n    with open(path_to_html, \"r\") as s:    \n        contents = s.read() \n    soup = BeautifulSoup(contents, 'html.parser')    \n    return soup\n\ndef save_msg_to_txt(soup, msg_id: str, output_path: str):\n    '''Функция для сохранения сообщения в текстовый файл'''\n    with open(output_path, 'w') as f:\n        try:\n            f.write(soup.body.find('div', id=msg_id).find_all('div', attrs={'class':\"text\"})[0].text)\n        except IndexError:\n            pass \n\ndef create_df_from_dict(dictionary: dict, output_dir: str, df_name:str): \n    '''Функция преобразования словаря в DataFrame'''\n    if len(dictionary) != 0:                         \n        df = pd.DataFrame({\n            'quest_id' : list(dictionary.keys()),\n            'ans_id': list(dictionary.values())})\n        df.to_csv(join(output_dir, df_name), index=False)\n\ndef parse_html(path_to_html: str):\n    '''Функция для парсинга html файла'''\n    \n    # считаем html-документ\n    soup = read_html(path_to_html)\n    \n    # словарь для хранения связей вопрос-ответ\n    this_html_connect_dict = {}\n     \n    file_name = path_to_html.split('/')[-1].split('.')[0]\n    connections_dir = join('..', 'data', 'connections_info')\n    output_dir = join('..', 'data', 'connections_info', file_name)\n    messages_dir = join('..', 'data', 'messages_text')\n    \n    # получим id всех сообщений в html-документе\n    msg_ids = [tag['id'] for tag in soup.select('div[id]')]\n    \n    # создаем необходимые папки\n    os.makedirs(messages_dir, exist_ok=True)\n    os.makedirs(connections_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    with open(join(output_dir, file_name + '_info.txt'), 'w') as f:\n        for elem in msg_ids:\n            f.writelines(elem+'\\n')\n            \n    for msg_id in msg_ids:\n        # проверяем содержит ли msg_id в себе тег 'a' \n        a_list = soup.body.find('div', id=msg_id).find_all('a')\n        if a_list != []:\n            for a_elem in a_list:\n                # если в тексте тега 'а' msg_id есть фраза 'this message', \n                # значит это сообщение является ответом на какое-то другое сообщение\n                if a_elem.text == 'this message':\n                    # получим id сообщения, на которое отвечали\n                    question_id = 'message' + re.findall(\"\\d+\", a_elem.get('href'))[0]\n                    # проверим есть ли сообщение, на которое отвечали (question_id), в текущем html-файле\n                    if question_id in msg_ids:\n                        quest_txt_path = join(messages_dir, question_id + '.txt')\n                        ans_txt_path = join(messages_dir, msg_id + '.txt')\n                        # создаем текстовый файл, только если его еще нет\n                        if not os.path.exists(quest_txt_path):\n                            save_msg_to_txt(soup=soup, msg_id=question_id, output_path=quest_txt_path)\n                                 \n                        if not os.path.exists(ans_txt_path):\n                            save_msg_to_txt(soup=soup, msg_id=msg_id, output_path=ans_txt_path)\n                            \n                        if (os.path.exists(quest_txt_path) + os.path.exists(ans_txt_path)) == 2:\n                            this_html_connect_dict[question_id] = msg_id\n\n    # создаем df содержащий связи вопрос-ответ\n    create_df_from_dict(dictionary=this_html_connect_dict,\n                        output_dir=output_dir,\n                        df_name='this_html_connect.csv')\n    \ndef get_qa_txt_file(df_path: str, msg_dir: str, output_path: str):\n    '''\n    Функция создания текстового файла в формате:\n    \n    Q: ##текст вопроса##\n    A: ##текст ответа##\n    '''\n    df = pd.read_csv(df_path)\n    with open(output_path, \"w\") as outfile:\n        for q, a in zip(df.quest_id.values,\n                        df.ans_id.values):\n            quest_path = join(msg_dir, q+'.txt')\n            ans_path = join(msg_dir, a+'.txt')\n            # проверяем существование txt файлов, а также наличие вопроса\n            if ((os.path.exists(quest_path) + os.path.exists(ans_path)) == 2) and \\\n               (get_str_from_file(quest_path).find('?') != -1) and \\\n               (len(get_str_from_file(ans_path)) > 3):\n                outfile.write('Q: ' + get_str_from_file(quest_path).strip() + '\\n')\n                outfile.write('A: ' + get_str_from_file(ans_path).strip() + '\\n')\n\ndef create_dataset(list_of_txt: list, output_path: str):\n    '''Функция создания датасета из нескольких текстовых файлов'''\n    with open(output_path, 'w') as outfile:\n        for fname in list_of_txt:\n            with open(fname) as infile:\n                for line in infile:\n                    outfile.write(line)","metadata":{"cellId":"4rpk27qx1iihlno7fjwnbp"},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Выполняем парсинг тг чата","metadata":{"cellId":"fdyyme9vmca19la5yfdbop"}},{"cell_type":"code","source":"#!c1.8\n%%time\nsys.setrecursionlimit(10000)\nfrom multiprocessing import Pool\nhtml_list = glob(join('..', 'data', 'ChatExport_Alice', '*.html'))\nwith Pool(8) as p:\n    p.map(parse_html, html_list)","metadata":{"cellId":"i924c8y5eksvd7uudq0b6s"},"outputs":[{"output_type":"stream","name":"stdout","text":"CPU times: user 8.54 s, sys: 3.83 s, total: 12.4 s\nWall time: 1h 16min 8s\n"},{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:840: UserWarning: The following variables cannot be serialized: p\n  warnings.warn(message)\n"}],"execution_count":41},{"cell_type":"code","source":"#!c1.8\nlen(html_list)","metadata":{"cellId":"b8slzwad8tmo5ozrobjv8l"},"outputs":[{"output_type":"display_data","data":{"text/plain":"359"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"#!c1.8\nhtml_list[0]","metadata":{"cellId":"td6xm65lk54jftn09wzsl"},"outputs":[{"output_type":"display_data","data":{"text/plain":"'../data/ChatExport_Alice/messages10.html'"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"#!c1.8\n%%time\nmessages_dir = join('..', 'data', 'messages_text')\nconnections_path = join('..', 'data', 'connections_info')\ndir_list = [join(connections_path, directory) for directory in os.listdir(connections_path)]\nfor dir_path in dir_list:\n    get_qa_txt_file(join(dir_path, 'this_html_connect.csv'), \n                    messages_dir,\n                    join(dir_path, dir_path.split('/')[-1] + '.txt'))","metadata":{"cellId":"rqs2kiwhvy95p6j2emcy","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"CPU times: user 20.8 s, sys: 21.3 s, total: 42.1 s\nWall time: 3min 49s\n"}],"execution_count":63},{"cell_type":"markdown","source":"### Создаем датасет\n","metadata":{"cellId":"8eif1gr5ravb47skykl1"}},{"cell_type":"code","source":"#!c1.8\n%%time\ntxt_list = [join(dir_path, dir_path.split('/')[-1] + '.txt') for dir_path in dir_list]\ntrain_size = round(len(txt_list)*0.8)\ncreate_dataset(list_of_txt=txt_list[:train_size], output_path=join('..', 'data','train_house.txt'))\ncreate_dataset(list_of_txt=txt_list[train_size:], output_path=join('..', 'data','valid_house.txt'))","metadata":{"cellId":"rnxqw6uc978u2039xfbtyn"},"outputs":[{"output_type":"stream","name":"stdout","text":"CPU times: user 128 ms, sys: 68 ms, total: 196 ms\nWall time: 1.74 s\n"}],"execution_count":62},{"cell_type":"code","source":"#!c1.8\n","metadata":{"cellId":"cip616hc6vq489xoy3vydg"},"outputs":[],"execution_count":null}]}