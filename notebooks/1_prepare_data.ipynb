{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","nbconvert_exporter":"python"},"notebookId":"0584e1f1-0d3c-458a-b527-c4acac5b3798","kernelspec":{"name":"python3","description":"IPython kernel implementation for Yandex DataSphere","spec":{"language":"python","display_name":"Yandex DataSphere Kernel","codemirror_mode":"python","argv":["/bin/true"],"env":{},"help_links":[]},"resources":{},"display_name":"Yandex DataSphere Kernel"},"ydsNotebookPath":"notebooks/1_prepare_data-Copy1.ipynb"},"cells":[{"cell_type":"markdown","source":["## Импорт библиотек"],"metadata":{"cellId":"ff85c56o9esh4kule0xrkb"}},{"cell_type":"code","source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import sys\n","from glob import glob\n","from os.path import join\n","import os\n","import re\n","import multiprocessing as mp\n","from joblib import Parallel, delayed\n","import numpy as np\n","%pip install lxml\n","\n","np.random.seed(42)"],"metadata":{"cellId":"pm1vpb7yr8qs6mrooap09"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Парсинг телеграм чата"],"metadata":{"cellId":"6cz0ckpeinbp4stexi0or"}},{"cell_type":"code","source":["def get_str_from_file(path_to_file: str) -> str:\n","    '''Функция для полуения строки из текстового файла'''\n","    with open(path_to_file) as f:\n","        contents = f.readlines()\n","    # находим слившиеся со словами ссылки и разделям их\n","    text = ' '.join(content.strip().replace('http', ' http') if content.strip().find('http') not in [-1, 0] else content.strip() for content in contents)    \n","    # удаляем ссылки из сообщений\n","    text_without_links = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)\n","    return text_without_links\n","\n","def read_html(path_to_html: str):\n","    '''Функция для чтения html'''\n","    with open(path_to_html, \"r\") as s:    \n","        contents = s.read() \n","    soup = BeautifulSoup(contents, 'html.parser')    \n","    return soup\n","\n","def save_msg_to_txt(soup, msg_id: str, output_path: str):\n","    '''Функция для сохранения сообщения в текстовый файл'''\n","    with open(output_path, 'w') as f:\n","        try:\n","            f.write(soup.body.find('div', id=msg_id).find_all('div', attrs={'class':\"text\"})[0].text)\n","        except IndexError:\n","            pass \n","\n","def create_df_from_dict(dictionary: dict, output_dir: str, df_name:str): \n","    '''Функция преобразования словаря в DataFrame'''\n","    if len(dictionary) != 0:                         \n","        df = pd.DataFrame({\n","            'quest_id' : list(dictionary.keys()),\n","            'ans_id': list(dictionary.values())})\n","        df.to_csv(join(output_dir, df_name), index=False)\n","\n","def parse_html(path_to_html: str):\n","    '''Функция для парсинга html файла'''\n","    \n","    # считаем html-документ\n","    soup = read_html(path_to_html)\n","    \n","    # словарь для хранения связей вопрос-ответ\n","    this_html_connect_dict = {}\n","     \n","    file_name = path_to_html.split('/')[-1].split('.')[0]\n","    connections_dir = join('..', 'data', 'connections_info')\n","    output_dir = join('..', 'data', 'connections_info', file_name)\n","    messages_dir = join('..', 'data', 'messages_text')\n","    \n","    # получим id всех сообщений в html-документе\n","    msg_ids = [tag['id'] for tag in soup.select('div[id]')]\n","    \n","    # создаем необходимые папки\n","    os.makedirs(messages_dir, exist_ok=True)\n","    os.makedirs(connections_dir, exist_ok=True)\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    with open(join(output_dir, file_name + '_info.txt'), 'w') as f:\n","        for elem in msg_ids:\n","            f.writelines(elem+'\\n')\n","            \n","    for msg_id in msg_ids:\n","        # проверяем содержит ли msg_id в себе тег 'a' \n","        a_list = soup.body.find('div', id=msg_id).find_all('a')\n","        if a_list != []:\n","            for a_elem in a_list:\n","                # если в тексте тега 'а' msg_id есть фраза 'this message', \n","                # значит это сообщение является ответом на какое-то другое сообщение\n","                if a_elem.text == 'this message':\n","                    # получим id сообщения, на которое отвечали\n","                    question_id = 'message' + re.findall(\"\\d+\", a_elem.get('href'))[0]\n","                    # проверим есть ли сообщение, на которое отвечали (question_id), в текущем html-файле\n","                    if question_id in msg_ids:\n","                        quest_txt_path = join(messages_dir, question_id + '.txt')\n","                        ans_txt_path = join(messages_dir, msg_id + '.txt')\n","                        # создаем текстовый файл, только если его еще нет\n","                        if not os.path.exists(quest_txt_path):\n","                            save_msg_to_txt(soup=soup, msg_id=question_id, output_path=quest_txt_path)\n","                                 \n","                        if not os.path.exists(ans_txt_path):\n","                            save_msg_to_txt(soup=soup, msg_id=msg_id, output_path=ans_txt_path)\n","                            \n","                        if (os.path.exists(quest_txt_path) + os.path.exists(ans_txt_path)) == 2:\n","                            this_html_connect_dict[question_id] = msg_id\n","\n","    # создаем df содержащий связи вопрос-ответ\n","    create_df_from_dict(dictionary=this_html_connect_dict,\n","                        output_dir=output_dir,\n","                        df_name='this_html_connect.csv')\n","    \n","def get_qa_txt_file(df_path: str, msg_dir: str, output_path: str):\n","    '''\n","    Функция создания текстового файла в формате:\n","    \n","    Q: ##текст вопроса##\n","    A: ##текст ответа##\n","    '''\n","    df = pd.read_csv(df_path)\n","    with open(output_path, \"w\") as outfile:\n","        for q, a in zip(df.quest_id.values,\n","                        df.ans_id.values):\n","            quest_path = join(msg_dir, q+'.txt')\n","            ans_path = join(msg_dir, a+'.txt')\n","            # проверяем существование txt файлов, а также наличие вопроса\n","            if ((os.path.exists(quest_path) + os.path.exists(ans_path)) == 2) and \\\n","               (get_str_from_file(quest_path).find('?') != -1) and \\\n","               (len(get_str_from_file(ans_path)) > 3):\n","                outfile.write('Q: ' + get_str_from_file(quest_path).strip() + '\\n')\n","                outfile.write('A: ' + get_str_from_file(ans_path).strip() + '\\n')\n","\n","def create_dataset(list_of_txt: list, output_path: str):\n","    '''Функция создания датасета из нескольких текстовых файлов'''\n","    with open(output_path, 'w') as outfile:\n","        for fname in list_of_txt:\n","            with open(fname) as infile:\n","                for line in infile:\n","                    outfile.write(line)"],"metadata":{"cellId":"4rpk27qx1iihlno7fjwnbp"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Выполняем парсинг тг чата"],"metadata":{"cellId":"fdyyme9vmca19la5yfdbop"}},{"cell_type":"code","source":["#!c1.8\n","%%time\n","sys.setrecursionlimit(10000)\n","from multiprocessing import Pool\n","html_list = glob(join('..', 'data', 'ChatExport', '*.html'))\n","with Pool(8) as p:\n","    p.map(parse_html, html_list)"],"metadata":{"cellId":"i924c8y5eksvd7uudq0b6s"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Создаем общий текстовый файл с вопросами и ответами для каждого html-файла"],"metadata":{"cellId":"51s5wkbfvqplek68a9zqu"}},{"cell_type":"code","source":["#!c1.8\n","%%time\n","messages_dir = join('..', 'data', 'messages_text')\n","connections_path = join('..', 'data', 'connections_info')\n","dir_list = [join(connections_path, directory) for directory in os.listdir(connections_path)]\n","for dir_path in dir_list:\n","    get_qa_txt_file(join(dir_path, 'this_html_connect.csv'), \n","                    messages_dir,\n","                    join(dir_path, dir_path.split('/')[-1] + '.txt'))"],"metadata":{"cellId":"rqs2kiwhvy95p6j2emcy"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Создаем датасет\n",""],"metadata":{"cellId":"8eif1gr5ravb47skykl1"}},{"cell_type":"code","source":["#!c1.8\n","%%time\n","os.makedirs(join('..', 'data', 'dataset'), exist_ok=True)\n","txt_list = [join(dir_path, dir_path.split('/')[-1] + '.txt') for dir_path in dir_list]\n","train_size = round(len(txt_list)*0.8)\n","create_dataset(list_of_txt=txt_list[:train_size], output_path=join('..', 'data', 'dataset','train.txt'))\n","create_dataset(list_of_txt=txt_list[train_size:], output_path=join('..', 'data', 'dataset','valid.txt'))"],"metadata":{"cellId":"rnxqw6uc978u2039xfbtyn"},"outputs":[],"execution_count":0}]}